# ğŸš€ Centralized Exchange (CEX)

A production-grade, event-driven cryptocurrency exchange built with scalable architecture, low-latency matching engine, and real-time market data streaming.

Designed with separation of concerns, high throughput, crash recovery, and financial consistency in mind.

---

## ğŸ§± Architecture

This exchange follows an **event-driven microservices architecture** powered by Kafka.

![Architecture](./images/architecture.png)

---

## ğŸ§  Core Design Principles

- âš¡ Low latency in-memory matching engine
- ğŸ“¦ Event-driven architecture (Kafka backbone)
- ğŸ” Idempotent & replayable event processing
- ğŸ’° Financial consistency via transactional settlement
- ğŸ“Š Real-time candle generation
- ğŸ”Œ WebSocket-based live market updates
- ğŸ›¡ Crash recovery using Redis snapshots
- ğŸ“ˆ Horizontally scalable per market

---

## ğŸ›  Tech Stack

### Monorepo

- Turborepo

### Backend

- Node.js
- Express.js
- Kafka
- Redis

### Frontend

- Next.js
- TanStack Query
- Tailwind CSS

### Database

- PostgreSQL
- Prisma ORM

---

## ğŸ–¼ï¸ Demo Images

![Dashboard](./images/dashboard.png)
![Market Page](./images/market-page.png)
![Portfolio](./images/portfolio.png)
![Ledgers](./images/ledgers.png)

---

# ğŸ“¦ Services Breakdown

## 1ï¸âƒ£ Primary Server

- Authentication (OTP based)
- Deposit handling
- Market & candle fetching
- Proxies order requests to Trading API

---

## 2ï¸âƒ£ Trading API

- Order validation
- Balance locking
- Writes order as `PENDING`
- Publishes:
  - `ORDER_CREATE`
  - `ORDER_CANCEL`

> No public access â€” internal only.

---

## 3ï¸âƒ£ Matching Engine

- Consumes order events from Kafka
- Maintains in-memory orderbook per market
- Emits:
  - `ORDER_OPENED`
  - `ORDERBOOK_SNAPSHOT`
  - `ORDERBOOK_UPDATED`
  - `ORDER_CANCELLED`
  - `TRADE_EXECUTED`
- Stores periodic snapshot in Redis
- Recovers state on crash

âš¡ Designed for high-speed execution.

---

## 4ï¸âƒ£ Settlement Service

- Consumes execution events
- Updates:
  - Order states
  - Trades
  - Wallet balances
- Uses DB transactions for atomicity

ğŸ’° Database remains financial source of truth.

---

## 5ï¸âƒ£ Order Lifecycle Worker

- Expires stale `PENDING` orders
- Emits `ORDER_EXPIRED`

---

## 6ï¸âƒ£ Candle Aggregate Worker

- Listens to `TRADE_EXECUTED`
- Generates OHLC candles (multiple intervals)
- Stores in Redis
- Publishes via Redis PubSub

---

## 7ï¸âƒ£ Candle Persist Worker

- Flushes Redis candles to PostgreSQL

---

## 8ï¸âƒ£ WebSocket Gateway

- Listen to Kafka Events for Orderbook Snapshot & Orderbook Updates
- Listens to Redis PubSub
- Streams:
  - Orderbook updates
  - Trades
  - Candles
- Scales horizontally

---

# Engine Crash Recovery Process

This document explains how the matching engine recovers from a crash without losing any order events, using Kafka replay + snapshot-based deduplication.

---

## The Problem

The engine snapshots orderbook state every **10 seconds**. If a crash happens at **t=15s**, the last snapshot is from **t=10s** â€” meaning 5 seconds of applied events are missing from the snapshot but exist in both Kafka and Redis.

A naive replay would fail because:

- Redis already has `processed:{eventId} = "1"` for those 5 seconds of events
- The dedup check would skip them
- The orderbook would never catch up

---

## The Solution

Store `processedEventIds` **inside the snapshot itself**, not just in Redis.
On recovery, use the snapshot's ID set as the dedup authority â€” not Redis.

---

## Recovery Flow

```
ğŸ’¥ Crash at t=15s
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Restore Snapshot (t=10s)        â”‚
â”‚                                         â”‚
â”‚  âœ… Orderbook state restored            â”‚
â”‚  âœ… processedEventIds loaded into       â”‚
â”‚     in-memory Set from snapshot         â”‚
â”‚  âœ… catchUpTargets set to last          â”‚
â”‚     committed Kafka offsets             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Kafka Replays t=10s â†’ t=15s         â”‚
â”‚                                         â”‚
â”‚  ğŸ” isRecovering = true                 â”‚
â”‚  ğŸ” Dedup checks in-memory Set          â”‚
â”‚     (NOT Redis)                         â”‚
â”‚  â­ï¸  Already-in-snapshot events skipped â”‚
â”‚  âœ… New events applied to orderbook     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Offset reaches catchUpTarget        â”‚
â”‚                                         â”‚
â”‚  ğŸŸ¢ isRecovering = false                â”‚
â”‚  ğŸ” Dedup switches back to Redis        â”‚
â”‚  âœ… Live traffic resumes normally       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Concrete Example

### Normal Operation â€” t=0s to t=10s

| Offset | Event ID  | Action             | Result            |
| ------ | --------- | ------------------ | ----------------- |
| 1      | ORDER-AAA | Buy 1 BTC @ 50000  | âœ… Applied        |
| 2      | ORDER-BBB | Sell 1 BTC @ 50000 | âœ… Trade executed |
| 3      | ORDER-CCC | Buy 2 BTC @ 49000  | âœ… Applied        |
| 4      | ORDER-DDD | Sell 1 BTC @ 49500 | âœ… Partial fill   |

**Orderbook at t=10s:**

```
BIDS:  49000 -> 1 BTC
ASKS:  (empty)
```

---

### Snapshot Saved at t=10s

```json
{
  "orderbook": {
    "bids": [{ "price": 49000, "quantity": 1 }],
    "asks": []
  },
  "lastCommittedOffsets": [
    { "topic": "orders.create", "partition": 0, "offset": "5" }
  ],
  "processedEventIds": ["ORDER-AAA", "ORDER-BBB", "ORDER-CCC", "ORDER-DDD"]
}
```

---

### t=10s to t=15s â€” Events After Snapshot

| Offset | Event ID  | Action             | Result           |
| ------ | --------- | ------------------ | ---------------- |
| 5      | ORDER-EEE | Buy 3 BTC @ 51000  | âœ… Applied       |
| 6      | ORDER-FFF | Sell 2 BTC @ 51000 | âœ… Partial trade |
| 7      | ORDER-GGG | Buy 1 BTC @ 48000  | âœ… Applied       |

> âš ï¸ Engine crashes at t=15s. Offsets 5â€“7 are in Redis and Kafka but NOT in the snapshot.

---

### On Restart â€” Replay

```
Seek Kafka â†’ orders.create:0 at offset 5

offset 5 â†’ ORDER-EEE
  in snapshot processedEventIds? âŒ NO  â†’  apply âœ…

offset 6 â†’ ORDER-FFF
  in snapshot processedEventIds? âŒ NO  â†’  apply âœ…

offset 7 â†’ ORDER-GGG
  in snapshot processedEventIds? âŒ NO  â†’  apply âœ…

offset 8 â†’ no more messages
  catchUpTarget reached â†’ isRecovering = false
  switch to Redis dedup â†’ live traffic resumes âœ…
```

**Orderbook after recovery â€” identical to t=15s state before crash:**

```
BIDS:  49000 -> 1 BTC  (ORDER-CCC remainder)
       48000 -> 1 BTC  (ORDER-GGG)
ASKS:  51000 -> 1 BTC  (ORDER-EEE remainder)
```

---

## Why Not Just Use Redis for Dedup on Replay?

| Scenario                           | Redis has it? | Snapshot has it? | Correct action      |
| ---------------------------------- | ------------- | ---------------- | ------------------- |
| Event baked into snapshot          | âœ… Yes        | âœ… Yes           | Skip (in orderbook) |
| Event after snapshot, before crash | âœ… Yes        | âŒ No            | **Must replay**     |
| Genuinely new event (live)         | âŒ No         | âŒ No            | Apply               |

Redis alone cannot distinguish between rows 1 and 2. The snapshot's `processedEventIds` can.

---

## Key Invariants

- **Kafka** is the source of truth for all events
- **Snapshots** are a performance optimization â€” they avoid replaying the entire Kafka log from the beginning
- **Redis** is a fast-path dedup cache for live traffic only
- **`processedEventIds` in the snapshot** is the authoritative dedup store for recovery
- The engine is fully recoverable as long as Kafka retains messages past the snapshot offset

---

## One-Line Summary

> The snapshot bakes in which eventIds are already applied. On recovery we replay from Kafka but use the snapshot's eventId set as the dedup authority â€” not Redis â€” because Redis contains IDs from after the snapshot too. Once we consume past the snapshot's offset, we flip back to Redis for live traffic.

# ğŸ”„ Order Flow (Detailed)

1. Client places order
2. Trading API validates & locks funds
3. Order written as `PENDING`
4. `ORDER_CREATE` event published to Kafka
5. Matching engine processes event
6. If matched â†’ emits `TRADE_EXECUTED`
7. Settlement updates DB atomically
8. WS gateway pushes real-time update

---

# ğŸ§© Event-Driven Design

### Why Kafka?

- Durable log storage
- Replay capability
- Partition ordering (per market)
- High throughput
- Decoupled services

Partition strategy: **by market**

Ensures strict order execution per trading pair.

---

# ğŸ” Consistency Model

- Matching Engine â†’ Execution authority
- Settlement â†’ Financial authority
- Database â†’ Balance & trade source of truth
- Kafka â†’ Event source of truth

Model: **Eventual consistency with strong financial guarantees**

---

# ğŸ“Š Real-Time Features

- Live orderbook updates
- Real-time trade stream
- Multi-interval candle generation
- WebSocket broadcasting

---

# ğŸ›¡ Reliability & Fault Tolerance

- Idempotent consumers
- Unique constraints for trades & orders
- Redis orderbook snapshots
- Kafka replay for recovery
- DB transactions for atomic balance updates
- Outbox pattern for reliable event publishing

---

# ğŸ“ˆ Scalability Strategy

- Kafka partitioned by market
- Matching engine scalable per market
- WS gateway horizontally scalable
- Redis used for low-latency broadcasting
- Read replicas for heavy read queries

---

# ğŸ§ª Future Improvements

- Move matching engine to Rust/Go for ultra-low latency
- Introduce ledger-based accounting
- Add risk engine
- Add advanced surveillance (wash trading detection)
- Horizontal DB sharding

---

# ğŸ¯ Why This Project?

This project demonstrates:

- Advanced system design knowledge
- Event-driven architecture
- Financial transaction handling
- Distributed system thinking
- Real-time data streaming
- Fault tolerance & crash recovery
- Production-grade backend design

---

# â­ If you find this interesting, consider giving it a star!
